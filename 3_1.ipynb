{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0bEhIqdGQbr2e+Fwxv9/C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/faberm321/building-generative-ai-services/blob/main/3_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KxJPd6_sVsZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a752fb52-2c76-482c-8084-18831f27a4ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n  pipe = load_text_model()\\n  output = generate_text(pipe, prompt)\\n  print(output)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import Pipeline, pipeline\n",
        "\n",
        "prompt = \"Cómo configurar un proyecto FastAPI?\"\n",
        "system_prompt = \"Te llamas FastAPI bot y eres un chatbot muy útil, responsable de enseñar FastAPI a tus usuarios. Responde siempre en formato Markdown.\"\n",
        "\n",
        "device_type = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device = torch.device(device_type)\n",
        "\n",
        "def load_text_model():\n",
        "  pipe = pipeline(\n",
        "      \"Generación de texto\",\n",
        "      model = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "      torch_dtype = torch.bfloat16,\n",
        "      device = device,\n",
        "  )\n",
        "  return pipe\n",
        "\n",
        "def generate_text(pipe: Pipeline, prompt: str, temperature: float = 0.7) -> str:\n",
        "  messages = [\n",
        "              {\"role\": \"system\", \"content\": system_prompt},\n",
        "              {\"role\": \"user\", \"content\": prompt},\n",
        "  ]\n",
        "  prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "  predictions = pipe(prompt, temperature=temperature, max_new_tokens=256, do_sample=True, top_k=50, top_p=0.95)\n",
        "\n",
        "  output = predictions[0][\"generated_text\"].split(\"</s>\\n<|assistant|>\\n\")[-1]\n",
        "  return output\n",
        "\"\"\"\n",
        "  pipe = load_text_model()\n",
        "  output = generate_text(pipe, prompt)\n",
        "  print(output)\n",
        "\"\"\""
      ]
    }
  ]
}